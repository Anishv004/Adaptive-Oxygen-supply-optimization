{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hr_20.csv',\n",
       " 'hr_25.csv',\n",
       " 'hr_30.csv',\n",
       " 'hr_35.csv',\n",
       " 'hr_40.csv',\n",
       " 'merged_20.csv',\n",
       " 'merged_25.csv',\n",
       " 'merged_30.csv',\n",
       " 'merged_35.csv',\n",
       " 'merged_40.csv',\n",
       " 'prt_20.csv',\n",
       " 'prt_25.csv',\n",
       " 'prt_30.csv',\n",
       " 'prt_35.csv',\n",
       " 'prt_40.csv',\n",
       " 'rsp_20.csv',\n",
       " 'rsp_25.csv',\n",
       " 'rsp_30.csv',\n",
       " 'rsp_35.csv',\n",
       " 'rsp_40.csv',\n",
       " 'spo_20.csv',\n",
       " 'spo_25.csv',\n",
       " 'spo_30.csv',\n",
       " 'spo_35.csv',\n",
       " 'spo_40.csv',\n",
       " 'spv_20.csv',\n",
       " 'spv_25.csv',\n",
       " 'spv_30.csv',\n",
       " 'spv_35.csv',\n",
       " 'spv_40.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"Data_Disc/217a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: Data_Disc\\217a\n",
      "Saved merged file for suffix 20 in Data_Disc\\217a\n",
      "Saved merged file for suffix 25 in Data_Disc\\217a\n",
      "Saved merged file for suffix 30 in Data_Disc\\217a\n",
      "Saved merged file for suffix 35 in Data_Disc\\217a\n",
      "Saved merged file for suffix 40 in Data_Disc\\217a\n",
      "Processing directory: Data_Disc\\218c\n",
      "Saved merged file for suffix 20 in Data_Disc\\218c\n",
      "Saved merged file for suffix 25 in Data_Disc\\218c\n",
      "Saved merged file for suffix 30 in Data_Disc\\218c\n",
      "Saved merged file for suffix 35 in Data_Disc\\218c\n",
      "Saved merged file for suffix 40 in Data_Disc\\218c\n",
      "Processing directory: Data_Disc\\219a\n",
      "Saved merged file for suffix 20 in Data_Disc\\219a\n",
      "Saved merged file for suffix 25 in Data_Disc\\219a\n",
      "Saved merged file for suffix 30 in Data_Disc\\219a\n",
      "Saved merged file for suffix 35 in Data_Disc\\219a\n",
      "Saved merged file for suffix 40 in Data_Disc\\219a\n",
      "Processing directory: Data_Disc\\223a\n",
      "Saved merged file for suffix 20 in Data_Disc\\223a\n",
      "Saved merged file for suffix 25 in Data_Disc\\223a\n",
      "Saved merged file for suffix 30 in Data_Disc\\223a\n",
      "Saved merged file for suffix 35 in Data_Disc\\223a\n",
      "Saved merged file for suffix 40 in Data_Disc\\223a\n",
      "Processing directory: Data_Disc\\223b\n",
      "Saved merged file for suffix 20 in Data_Disc\\223b\n",
      "Saved merged file for suffix 25 in Data_Disc\\223b\n",
      "Saved merged file for suffix 30 in Data_Disc\\223b\n",
      "Saved merged file for suffix 35 in Data_Disc\\223b\n",
      "Saved merged file for suffix 40 in Data_Disc\\223b\n",
      "Processing directory: Data_Disc\\310a\n",
      "Saved merged file for suffix 20 in Data_Disc\\310a\n",
      "Saved merged file for suffix 25 in Data_Disc\\310a\n",
      "Saved merged file for suffix 30 in Data_Disc\\310a\n",
      "Saved merged file for suffix 35 in Data_Disc\\310a\n",
      "Saved merged file for suffix 40 in Data_Disc\\310a\n",
      "Processing directory: Data_Disc\\310b\n",
      "Saved merged file for suffix 20 in Data_Disc\\310b\n",
      "Saved merged file for suffix 25 in Data_Disc\\310b\n",
      "Saved merged file for suffix 30 in Data_Disc\\310b\n",
      "Saved merged file for suffix 35 in Data_Disc\\310b\n",
      "Saved merged file for suffix 40 in Data_Disc\\310b\n",
      "Processing directory: Data_Disc\\314b\n",
      "Saved merged file for suffix 20 in Data_Disc\\314b\n",
      "Saved merged file for suffix 25 in Data_Disc\\314b\n",
      "Saved merged file for suffix 30 in Data_Disc\\314b\n",
      "Saved merged file for suffix 35 in Data_Disc\\314b\n",
      "Saved merged file for suffix 40 in Data_Disc\\314b\n",
      "Processing directory: Data_Disc\\314c\n",
      "Saved merged file for suffix 20 in Data_Disc\\314c\n",
      "Saved merged file for suffix 25 in Data_Disc\\314c\n",
      "Saved merged file for suffix 30 in Data_Disc\\314c\n",
      "Saved merged file for suffix 35 in Data_Disc\\314c\n",
      "Saved merged file for suffix 40 in Data_Disc\\314c\n",
      "Processing directory: Data_Disc\\318a\n",
      "Saved merged file for suffix 20 in Data_Disc\\318a\n",
      "Saved merged file for suffix 25 in Data_Disc\\318a\n",
      "Saved merged file for suffix 30 in Data_Disc\\318a\n",
      "Saved merged file for suffix 35 in Data_Disc\\318a\n",
      "Saved merged file for suffix 40 in Data_Disc\\318a\n",
      "Processing directory: Data_Disc\\318b\n",
      "Saved merged file for suffix 20 in Data_Disc\\318b\n",
      "Saved merged file for suffix 25 in Data_Disc\\318b\n",
      "Saved merged file for suffix 30 in Data_Disc\\318b\n",
      "Saved merged file for suffix 35 in Data_Disc\\318b\n",
      "Saved merged file for suffix 40 in Data_Disc\\318b\n",
      "Processing directory: Data_Disc\\318c\n",
      "Saved merged file for suffix 20 in Data_Disc\\318c\n",
      "Saved merged file for suffix 25 in Data_Disc\\318c\n",
      "Saved merged file for suffix 30 in Data_Disc\\318c\n",
      "Saved merged file for suffix 35 in Data_Disc\\318c\n",
      "Saved merged file for suffix 40 in Data_Disc\\318c\n",
      "Processing directory: Data_Disc\\321a\n",
      "Saved merged file for suffix 20 in Data_Disc\\321a\n",
      "Saved merged file for suffix 25 in Data_Disc\\321a\n",
      "Saved merged file for suffix 30 in Data_Disc\\321a\n",
      "Saved merged file for suffix 35 in Data_Disc\\321a\n",
      "Saved merged file for suffix 40 in Data_Disc\\321a\n",
      "Processing directory: Data_Disc\\328a\n",
      "Saved merged file for suffix 20 in Data_Disc\\328a\n",
      "Saved merged file for suffix 25 in Data_Disc\\328a\n",
      "Saved merged file for suffix 30 in Data_Disc\\328a\n",
      "Saved merged file for suffix 35 in Data_Disc\\328a\n",
      "hr_40.csv not found in Data_Disc\\328a\n",
      "Processing directory: Data_Disc\\328b\n",
      "Saved merged file for suffix 20 in Data_Disc\\328b\n",
      "Saved merged file for suffix 25 in Data_Disc\\328b\n",
      "Saved merged file for suffix 30 in Data_Disc\\328b\n",
      "Saved merged file for suffix 35 in Data_Disc\\328b\n",
      "Saved merged file for suffix 40 in Data_Disc\\328b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the main directory containing subdirectories\n",
    "main_dir = 'Data_Disc'\n",
    "\n",
    "# List of required prefixes and suffixes\n",
    "prefixes = ['hr_', 'prt_', 'spv_']\n",
    "suffixes = ['20', '25', '30', '35', '40']\n",
    "\n",
    "# Function to merge CSV files based on timestamp\n",
    "def merge_files(subdir_path, suffix):\n",
    "    # Dictionary to store dataframes with the required prefixes\n",
    "    dfs = {}\n",
    "    \n",
    "    for prefix in prefixes:\n",
    "        file_name = f\"{prefix}{suffix}.csv\"\n",
    "        file_path = os.path.join(subdir_path, file_name)\n",
    "        \n",
    "        # Read CSV file assuming no header, assign timestamp and data column names dynamically\n",
    "        if os.path.exists(file_path):\n",
    "            df = pd.read_csv(file_path, header=None)\n",
    "            df.columns = ['timestamp', file_name.split('.')[0]]  # Assign 'timestamp' and file-specific header\n",
    "            # df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d-%m-%Y %H:%M:%S')  # Parse timestamp\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "            df.set_index('timestamp', inplace=True)  # Set timestamp as index\n",
    "            dfs[prefix] = df\n",
    "        else:\n",
    "            print(f\"{file_name} not found in {subdir_path}\")\n",
    "            return\n",
    "    \n",
    "    # Merge all the dataframes on the 'timestamp'\n",
    "    merged_df = dfs['hr_'].join([dfs['prt_'], dfs['spv_']], how='inner')\n",
    "    \n",
    "    # Save the merged file in the subdirectory\n",
    "    output_file = os.path.join(subdir_path, f'merged_{suffix}.csv')\n",
    "    merged_df.to_csv(output_file)\n",
    "    print(f\"Saved merged file for suffix {suffix} in {subdir_path}\")\n",
    "\n",
    "# Iterate through each subdirectory\n",
    "for subdir in os.listdir(main_dir):\n",
    "    subdir_path = os.path.join(main_dir, subdir)\n",
    "    \n",
    "    if os.path.isdir(subdir_path):\n",
    "        print(f\"Processing directory: {subdir_path}\")\n",
    "        \n",
    "        # Merge files for each suffix\n",
    "        for suffix in suffixes:\n",
    "            merge_files(subdir_path, suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined file for suffix 20 in Data_Disc\n",
      "Saved combined file for suffix 25 in Data_Disc\n",
      "Saved combined file for suffix 30 in Data_Disc\n",
      "Saved combined file for suffix 35 in Data_Disc\n",
      "merged_40.csv not found in Data_Disc\\328a\n",
      "Saved combined file for suffix 40 in Data_Disc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the main directory containing subdirectories\n",
    "main_dir = 'Data_Disc'\n",
    "\n",
    "# List of suffixes to merge\n",
    "suffixes = ['20', '25', '30', '35', '40']\n",
    "\n",
    "# Function to merge all merged_<suffix>.csv files\n",
    "def merge_all_files(suffix):\n",
    "    all_dataframes = []\n",
    "    \n",
    "    # Iterate through each subdirectory\n",
    "    for subdir in os.listdir(main_dir):\n",
    "        subdir_path = os.path.join(main_dir, subdir)\n",
    "        \n",
    "        if os.path.isdir(subdir_path):\n",
    "            file_name = f'merged_{suffix}.csv'\n",
    "            file_path = os.path.join(subdir_path, file_name)\n",
    "            \n",
    "            # Read the merged file and add a new column for the subdirectory name\n",
    "            if os.path.exists(file_path):\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S')\n",
    "                df['participant'] = subdir  # Add subdirectory name as a new column\n",
    "                all_dataframes.append(df)\n",
    "            else:\n",
    "                print(f\"{file_name} not found in {subdir_path}\")\n",
    "    \n",
    "    # Concatenate all dataframes row-wise\n",
    "    if all_dataframes:\n",
    "        combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        # Save the combined dataframe\n",
    "        output_file = os.path.join(main_dir, f'combined_merged_{suffix}.csv')\n",
    "        combined_df.to_csv(output_file, index=False)\n",
    "        print(f\"Saved combined file for suffix {suffix} in {main_dir}\")\n",
    "\n",
    "# Iterate through each suffix to merge the files\n",
    "for suffix in suffixes:\n",
    "    merge_all_files(suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hr</th>\n",
       "      <th>prt</th>\n",
       "      <th>spv</th>\n",
       "      <th>participant</th>\n",
       "      <th>altitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-17 09:51:30</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217a</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-17 09:51:31</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.729167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217a</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-17 09:51:32</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>217a</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-17 09:51:33</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>217a</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-17 09:51:34</td>\n",
       "      <td>0.830769</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>217a</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp        hr       prt       spv participant  altitude\n",
       "0  2023-02-17 09:51:30  0.653846  0.729167  0.000000        217a       4.0\n",
       "1  2023-02-17 09:51:31  0.653846  0.729167  0.000000        217a       4.0\n",
       "2  2023-02-17 09:51:32  0.653846  0.750000  0.000000        217a       4.0\n",
       "3  2023-02-17 09:51:33  0.700000  0.750000  0.028571        217a       4.0\n",
       "4  2023-02-17 09:51:34  0.830769  0.770833  0.085714        217a       4.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define file paths and corresponding altitudes\n",
    "file_paths = [\n",
    "    ('Data_Disc/combined_merged_20.csv', 2.0),   # 2.0 km altitude for combined_merged_20.csv\n",
    "    ('Data_Disc//combined_merged_25.csv', 2.5),   # 2.5 km altitude for combined_merged_25.csv\n",
    "    ('Data_Disc/combined_merged_30.csv', 3.0),   # 3.0 km altitude for combined_merged_30.csv\n",
    "    ('Data_Disc/combined_merged_35.csv', 3.5),   # 3.5 km altitude for combined_merged_35.csv\n",
    "    ('Data_Disc/combined_merged_40.csv', 4.0)    # 4.0 km altitude for combined_merged_40.csv\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "# Function to rename columns by removing the altitude suffix (_20, _25, etc.)\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace(r'_\\d+', '', regex=True)  # Remove suffixes like _20, _25\n",
    "    return df\n",
    "\n",
    "# Loop over each file and its corresponding altitude\n",
    "for file_path, altitude in file_paths:\n",
    "    df = pd.read_csv(file_path)    # Read the csv file\n",
    "    df = clean_column_names(df)    # Clean column names by removing altitude-specific suffixes\n",
    "    df['altitude'] = altitude      # Add a new column for altitude\n",
    "    dataframes.append(df)          # Append dataframe to the list\n",
    "\n",
    "# Concatenate all dataframes row-wise\n",
    "merged_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "merged_data['hr'] = merged_data['hr']*250\n",
    "merged_data['prt'] = merged_data['prt']*250\n",
    "merged_data['spv'] = merged_data['spv']*100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiO2 = baseline_FiO2 + α1 × (92−spv) + α2× altitude + α3 × (hr−70)+ α4 × (prt−70)\n",
    "\n",
    "α1 : Altitude effect (e.g., 0.01 per 100 meters)\n",
    "\n",
    "α2 : SpO2 effect (e.g., 0.5 per 1% decrease below 92%)\n",
    "\n",
    "α3 : Heart rate effect (e.g., 0.2 per bpm above 70)\n",
    "\n",
    "α4 : Pulse rate effect (e.g., 0.2 per bpm above 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "merged_data = merged_data[merged_data['spv'] != 0]\n",
    "merged_data['fio2'] = 33.39 / (1013.25 * np.exp(-(merged_data['altitude'] * 1000) / 8400))\n",
    "merged_data['inter'] = merged_data.apply(lambda row: row['fio2'] * (90 / row['spv']) if row['spv'] < 90 else row['fio2'], axis=1)\n",
    "# merged_data['fio2'] = 1 / (1013.25 * np.exp(-(merged_data['altitude'] * 1000) / 8400))\n",
    "# merged_data['inter'] = merged_data.apply(lambda row: row['fio2'] * (90 / row['spv']) if row['spv'] < 90 else row['fio2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['target'] = merged_data['inter'] * (1 + (merged_data['hr'] - 72) / 100)\n",
    "df = merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>hr</th>\n",
       "      <th>prt</th>\n",
       "      <th>spv</th>\n",
       "      <th>participant</th>\n",
       "      <th>altitude</th>\n",
       "      <th>fio2</th>\n",
       "      <th>inter</th>\n",
       "      <th>target</th>\n",
       "      <th>FiO2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-02-17 09:16:12</td>\n",
       "      <td>159.615385</td>\n",
       "      <td>135.416667</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>217a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.082368</td>\n",
       "      <td>56.503205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-17 09:16:13</td>\n",
       "      <td>151.923077</td>\n",
       "      <td>130.208333</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>217a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.078991</td>\n",
       "      <td>55.213141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-17 09:16:14</td>\n",
       "      <td>151.923077</td>\n",
       "      <td>130.208333</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>217a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.078991</td>\n",
       "      <td>55.213141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-17 09:16:15</td>\n",
       "      <td>136.538462</td>\n",
       "      <td>130.208333</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>217a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.072237</td>\n",
       "      <td>53.674679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-17 09:16:16</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>130.208333</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>217a</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.041812</td>\n",
       "      <td>0.043903</td>\n",
       "      <td>0.067171</td>\n",
       "      <td>52.520833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp          hr         prt        spv participant  \\\n",
       "0  2023-02-17 09:16:12  159.615385  135.416667  85.714286        217a   \n",
       "1  2023-02-17 09:16:13  151.923077  130.208333  85.714286        217a   \n",
       "2  2023-02-17 09:16:14  151.923077  130.208333  85.714286        217a   \n",
       "3  2023-02-17 09:16:15  136.538462  130.208333  85.714286        217a   \n",
       "4  2023-02-17 09:16:16  125.000000  130.208333  85.714286        217a   \n",
       "\n",
       "   altitude      fio2     inter    target       FiO2  \n",
       "0       2.0  0.041812  0.043903  0.082368  56.503205  \n",
       "1       2.0  0.041812  0.043903  0.078991  55.213141  \n",
       "2       2.0  0.041812  0.043903  0.078991  55.213141  \n",
       "3       2.0  0.041812  0.043903  0.072237  53.674679  \n",
       "4       2.0  0.041812  0.043903  0.067171  52.520833  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set baseline FiO2 (ambient air is 21%)\n",
    "baseline_FiO2 = 21\n",
    "\n",
    "# Define coefficients for each factor (you can adjust these values based on tuning)\n",
    "alpha_1 = 0.5  # Weight for SpO2 deviation\n",
    "alpha_2 = 0.01  # Weight for altitude (adjust for altitude unit, e.g., per 100m)\n",
    "alpha_3 = 0.1  # Weight for heart rate deviation\n",
    "alpha_4 = 0.1  # Weight for pulse rate deviation\n",
    "\n",
    "# Calculate FiO2 based on the formula\n",
    "df['FiO2'] = baseline_FiO2 + (\n",
    "    alpha_1 * (92 - np.maximum(92,df['spv'])) +  # SpO2 adjustment\n",
    "    alpha_2 * df['altitude'] * 1000 +  # Altitude adjustment (convert to meters if needed)\n",
    "    alpha_3 * (df['hr'] - 70) +        # Heart rate adjustment\n",
    "    alpha_4 * (df['prt'] - 70)         # Pulse rate adjustment\n",
    ")\n",
    "df['FiO2'] = np.clip(df['FiO2'], 0, 100)\n",
    "\n",
    "# Display the result\n",
    "# print(df[['hr', 'prt', 'spv', 'altitude', 'FiO2']])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data with cleaned column names and altitudes saved to merged_with_altitude_and_FiO2.csv\n"
     ]
    }
   ],
   "source": [
    "# Save the merged data to a new CSV file\n",
    "output_file = 'merged_with_altitude_and_FiO2.csv'\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Merged data with cleaned column names and altitudes saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
